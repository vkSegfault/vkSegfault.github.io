<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="layout-lang" content="en"><meta name="day-prompt" content="d ago"><meta name="hour-prompt" content="hr ago"><meta name="minute-prompt" content="min ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="SIMD usage in C++, C# and Rust" /><meta property="og:locale" content="en" /><meta name="description" content="Everyone heard of SIMD, yet not so many use it. There are probably three main reasons for that: it’s hard to write it correctly, it’s not safe and, most important, compiler can write it for us in many cases (auto vectorization). So why even bother? And answer is: bacause compilers are not that smart and for many performance-critical paths we can’t just rely on the will of compiler. You can easily break compiler-deduced vectorization by using break or goto, when number of iterations are not known at compile time, where there are dependencies between iterations or when number of elements is not aligned to your SIMD vector width. So let’s defy the odds and write intrinsics explicitly. Below is the whirlwind tour of SIMD usage in 3 popular languages." /><meta property="og:description" content="Everyone heard of SIMD, yet not so many use it. There are probably three main reasons for that: it’s hard to write it correctly, it’s not safe and, most important, compiler can write it for us in many cases (auto vectorization). So why even bother? And answer is: bacause compilers are not that smart and for many performance-critical paths we can’t just rely on the will of compiler. You can easily break compiler-deduced vectorization by using break or goto, when number of iterations are not known at compile time, where there are dependencies between iterations or when number of elements is not aligned to your SIMD vector width. So let’s defy the odds and write intrinsics explicitly. Below is the whirlwind tour of SIMD usage in 3 popular languages." /><link rel="canonical" href="https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/" /><meta property="og:url" content="https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/" /><meta property="og:site_name" content="vkSegfault" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-06T00:00:00+02:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="SIMD usage in C++, C# and Rust" /><meta name="twitter:site" content="@vksegfault" /><meta name="google-site-verification" content="googlebb3509c3bdf89758" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-13T14:42:16+02:00","datePublished":"2021-08-06T00:00:00+02:00","description":"Everyone heard of SIMD, yet not so many use it. There are probably three main reasons for that: it’s hard to write it correctly, it’s not safe and, most important, compiler can write it for us in many cases (auto vectorization). So why even bother? And answer is: bacause compilers are not that smart and for many performance-critical paths we can’t just rely on the will of compiler. You can easily break compiler-deduced vectorization by using break or goto, when number of iterations are not known at compile time, where there are dependencies between iterations or when number of elements is not aligned to your SIMD vector width. So let’s defy the odds and write intrinsics explicitly. Below is the whirlwind tour of SIMD usage in 3 popular languages.","headline":"SIMD usage in C++, C# and Rust","mainEntityOfPage":{"@type":"WebPage","@id":"https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/"},"url":"https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/"}</script><title>SIMD usage in C++, C# and Rust | vkSegfault</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="vkSegfault"><meta name="application-name" content="vkSegfault"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://vksegfault.github.io/assets/peng_hack.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">vkSegfault</a></div><div class="site-subtitle font-italic">Adrian Jurczak</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://twitter.com/vksegfault" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href="https://www.linkedin.com/in/adrian-jurczak/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['a.jurczak','protonmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>SIMD usage in C++, C# and Rust</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>SIMD usage in C++, C# and Rust</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Adrian Jurczak </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Aug 6, 2021, 12:00 AM +0200" >Aug 6, 2021<i class="unloaded">2021-08-06T00:00:00+02:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Oct 13, 2021, 2:42 PM +0200" >Oct 13, 2021<i class="unloaded">2021-10-13T14:42:16+02:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2053 words">11 min read</span></div></div><div class="post-content"><p>Everyone heard of SIMD, yet not so many use it. There are probably three main reasons for that: it’s hard to write it correctly, it’s not safe and, most important, compiler can write it for us in many cases (auto vectorization). So why even bother? And answer is: bacause compilers are not that smart and for many performance-critical paths we can’t just rely on the will of compiler. You can easily break compiler-deduced vectorization by using <code class="language-plaintext highlighter-rouge">break</code> or <code class="language-plaintext highlighter-rouge">goto</code>, when number of iterations are not known at compile time, where there are dependencies between iterations or when number of elements is not aligned to your SIMD vector width. So let’s defy the odds and write intrinsics explicitly. Below is the whirlwind tour of SIMD usage in 3 popular languages.</p><blockquote><p>Note that this blogpost is pure language usage comparasion, not benchmark. There is a lot of benchmarks out there so there is no need to repeat them.</p></blockquote><blockquote><p>C++ code was tested on <strong>GCC 11.1.0</strong>, C# on <strong>.NET 5.0.203</strong> and Rust on <strong>rustc 1.52.1</strong>. Everything compiled on Linux with small exception to C# when easy way to verify JITted assembly was VS Code plugin that runs only on Windows.</p></blockquote><h2 id="standarization">Standarization</h2><p>C++ has no standarized SIMD usage at all, even for SSE instructions introduced 20 years ago, which are available for almost all x86_64 CPUs as of 2021. To use wide lanes we must provide compiler-specific headers and use all instructions directly. In case of GCC it all looks like this:</p><div lang="cpp" class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;xmmintrin.h&gt;</span><span class="c1">   //for SSE</span><span class="cp">
#include</span> <span class="cpf">&lt;emmintrin.h&gt;</span><span class="c1">   //SSE2</span><span class="cp">
#include</span> <span class="cpf">&lt;pmmintrin.h&gt;</span><span class="c1">   //SSE3</span><span class="cp">
#include</span> <span class="cpf">&lt;smmintrin.h&gt;</span><span class="c1">   //SSE4.1</span><span class="cp">
#include</span> <span class="cpf">&lt;nmmintrin.h&gt;</span><span class="c1">   //SSE4.2</span><span class="cp">
#include</span> <span class="cpf">&lt;immintrin.h&gt;</span><span class="c1">   //AVX, AVX2, AVX-512</span><span class="cp">
</span></pre></table></code></div></div><p>All SSE includes give us <code class="language-plaintext highlighter-rouge">__m128</code> types while <code class="language-plaintext highlighter-rouge">immintrin.h</code> results in <code class="language-plaintext highlighter-rouge">__m256</code> and <code class="language-plaintext highlighter-rouge">__m512</code>. Although we have only this raw approach for now there is some work happening to standarize it in <a href="https://en.cppreference.com/w/cpp/experimental/simd">future</a>. It’s worth mentioning that in the 90’s <code class="language-plaintext highlighter-rouge">valarray</code> has been introduced. Where vector and array are just templated containers without any mathematical meaning, vallarray was concept of array containing numbers only, which can execute wide operations. Unfortunatelly development stalled and till today there is no exact matching to SIMD inctructions, so we must rely on auto vectorization.</p><p>At the other end of the spectrum there have been C# which had fully standarized SIMD usage ever since .NET Core 3.0. We don’t have to bother about any intrinsics directly, everything is under nice high level constructs:</p><div lang="c#" class="language-c# highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="k">using</span> <span class="nn">System.Numerics</span><span class="p">;</span>
</pre></table></code></div></div><p>From now on you can use types such a <code class="language-plaintext highlighter-rouge">Vector2</code>, <code class="language-plaintext highlighter-rouge">Vector3</code>, <code class="language-plaintext highlighter-rouge">Vector4</code> and <code class="language-plaintext highlighter-rouge">Matrix3x2</code>, <code class="language-plaintext highlighter-rouge">Matrix4x4</code>. There are even more specialized types like <code class="language-plaintext highlighter-rouge">Plane</code> or <code class="language-plaintext highlighter-rouge">Quaternion</code>. All those use single precission floating point numbers. If you want use doubles or ints and wider operations you can use <code class="language-plaintext highlighter-rouge">Vector&lt;T&gt;</code> type which also let you write hardware independent code.</p><blockquote><p>While .NET Core has SIMD out of the box, if you still use .NET Framework you need <strong><em>System.Numerics.Vectors</em></strong> NuGet package.</p></blockquote><p>And there is Rust, the new kid on the block. Given it’s pretty young (1.0 released in 2015), it should handle intrinsics in some meaningful way, right? Actually, the situation is a bit more complicated. Since 2018 Edition (1.27 version to be precise) we can use SIMD basically directly (similar to C++), but at the same time it’s all nicely wrapped into standard library:</p><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">arch</span><span class="p">::</span><span class="nn">x86_64</span><span class="p">::</span><span class="o">*</span><span class="p">;</span>
</pre></table></code></div></div><p>Ever since SIMD arch module introduction there was always some talk about creating portable (rusty) way to achieve wide lane instructions directly without compromising Rust safety and syntax. I’ve checked 2 previous attempts: <a href="https://crates.io/crates/simd">simd</a> and it’s successor <a href="https://crates.io/crates/packed_simd">packed simd</a>, but they are outdated and none of them compile as of rustc 1.51. The latest is <a href="https://blog.rust-lang.org/inside-rust/2020/09/29/Portable-SIMD-PG.html">Portable SIMD Project Group</a>. Once (or rather <em>if</em>) they finish we should be able to use constructs like <code class="language-plaintext highlighter-rouge">f32x4</code> directly with all mathematical operations on them.</p><blockquote><p>For C++ and Rust this list of SIMD operations will definitely come in handy: <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide">Intel Intrinsics Guide</a></p></blockquote><h2 id="safety">Safety</h2><p>As you can blindly guess, C++ way of CPU intrinsics is not safe in any way. What will happen when you run instructions on hardware that does not support it is UB, and most probably segfault. There is a way though to ensure runtime safety, but once more not portable. It’s called <strong>CPUID</strong> and here is an example:</p><div lang="cpp" class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="cp">#include</span>  <span class="cpf">&lt;cpuid.h&gt;</span><span class="cp">
</span>
<span class="c1">//(...)</span>

<span class="kt">int</span> <span class="n">info</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
<span class="n">__cpuid_count</span><span class="p">(</span><span class="mh">0x0000'0001</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">info</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">info</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">info</span><span class="p">[</span><span class="mi">3</span><span class="p">]);</span>
<span class="k">if</span> <span class="p">(</span> <span class="n">info</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="kt">int</span><span class="p">)</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">28</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="p">)</span> <span class="c1">//check bit for AVX presence</span>
<span class="p">{</span>
	<span class="c1">//do something with AVX</span>
<span class="p">}</span>
</pre></table></code></div></div><p>C# on the other hand does everything fully safe and potentially faster (!). At least theoretically (depends whether JITted code optimized for specific hardware gains more than CLR runtime workload<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). This code checks at runtime how wide (how many lanes) is the vector of SIMD data:</p><div lang="c#" class="language-c# highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kt">var</span> <span class="n">lanes</span> <span class="p">=</span> <span class="n">Vector</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;.</span><span class="n">Count</span><span class="p">;</span>
</pre></table></code></div></div><p>Of course, it implicitly matches to SSE (4 lanes), AVX (8 lanes), AVX-512 (16 lanes) or - in case of no SIMD inctructions detected - scalar (1 lane). While C++ and Rust must compile AOT and assume what is typical hardware your program will run on, C# doesn’t need to. CLR compiles it dynamically checking on what CPU it’s running. If it finds that we are using CPU modern enough to support e.g.: AVX-512, why bother with something less wide? This is really elegant way to acomplish portable SIMD usage comparing to C++ and Rust which must struggle with compile time decisions.</p><p>Rust has some neat syntax to create different functions per every instruction family and then dynamically decide which one to run. First to hint <strong>rustc</strong> to compile some function with avx we need feature attribute:</p><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nd">#[target_feature(enable</span> <span class="nd">=</span> <span class="s">"avx"</span><span class="nd">)]</span>   <span class="c1">//note that rustflags have precedence over this attribute</span>
</pre></table></code></div></div><p>and we can prepare many other functions for every use case e.g.: SSE, AVX, AVX-512 and fallback scalar versions compiled right away. Then we can check at run time which features our hardware supports and run specific path based on it:</p><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="nd">is_x86_feature_detected!</span><span class="p">(</span> <span class="s">"avx"</span> <span class="p">)</span> <span class="p">{</span>
</pre></table></code></div></div><p>We can also check statically whether particular target feature is enabled</p><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nd">#[cfg(target_feature</span> <span class="nd">=</span> <span class="s">"avx"</span><span class="nd">)]</span>
<span class="k">fn</span> <span class="nf">foo</span><span class="p">()</span> <span class="p">{</span>   <span class="c1">//foo is only compiled when avx is enabled</span>
<span class="nd">#[cfg(not(target_feature</span> <span class="nd">=</span> <span class="s">"avx"</span><span class="nd">))]</span>
<span class="k">fn</span> <span class="nf">foo</span><span class="p">()</span> <span class="p">{</span>   <span class="c1">//this foo is only compiled when avx is not available</span>

<span class="c1">//or via macro:</span>
<span class="k">if</span> <span class="nd">cfg!</span><span class="p">(</span><span class="n">target_feature</span> <span class="o">=</span> <span class="s">"avx"</span><span class="p">)</span> <span class="p">{</span>
</pre></table></code></div></div><blockquote><p>Small tidbit here: above runtime check in Rust is actually made by the same solution presented already in C++ part: CPUID.</p></blockquote><p>Please note that all fn’s that use <strong>std::arch</strong> module must be marked as <strong>unsafe</strong>!</p><h2 id="example">Example</h2><p>Here we have simple loop example for every language that sums 256 bits vector from a big array. All examples below are also hosted on <a href="https://gitlab.com/adtofaust/simd-comparison">Gitlab</a>.</p><blockquote><p>For the sake of simplicity below code snippets don’t handle cases where array is not multiple of SIMD width (thus array’s size is set on purpose to be multiple of 8). In production-ready code you need either add padding to array to match SIMD width or handle not aligned end of array with second loop using scalar version.</p></blockquote><ul><li>C++</ul><div lang="cpp" class="language-cpp highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">__m256</span>  <span class="n">v8temp</span><span class="p">;</span>
<span class="k">union</span> <span class="p">{</span> <span class="n">__m256</span>  <span class="n">v8sum</span><span class="p">;</span> <span class="kt">float</span>  <span class="n">f8sum</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span> <span class="p">};</span>
<span class="n">v8temp</span> <span class="o">=</span> <span class="n">v8sum</span> <span class="o">=</span> <span class="n">_mm256_setzero_ps</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span> <span class="mi">1'200'000</span><span class="o">&gt;</span> <span class="n">arr</span> <span class="p">{};</span>
<span class="n">arr</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span> <span class="mf">23.74</span> <span class="p">);</span>

<span class="k">for</span><span class="p">(</span> <span class="kt">int</span>  <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">arr</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">+=</span><span class="mi">8</span> <span class="p">)</span>
<span class="p">{</span>
	<span class="n">v8temp</span> <span class="o">=</span> <span class="n">_mm256_set_ps</span><span class="p">(</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">7</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">6</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">5</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">]</span> <span class="p">);</span>
	<span class="n">v8sum</span> <span class="o">=</span> <span class="n">_mm256_add_ps</span><span class="p">(</span> <span class="n">v8sum</span><span class="p">,</span> <span class="n">v8temp</span> <span class="p">);</span>
<span class="p">}</span>
</pre></table></code></div></div><ul><li>C#</ul><div lang="c#" class="language-c# highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="kt">var</span> <span class="n">lanes</span> <span class="p">=</span> <span class="n">Vector</span><span class="p">&lt;</span><span class="kt">int</span><span class="p">&gt;.</span><span class="n">Count</span><span class="p">;</span>

<span class="kt">float</span><span class="p">[]</span> <span class="n">arr</span> <span class="p">=</span> <span class="k">new</span>  <span class="kt">float</span><span class="p">[</span><span class="m">1_200_000</span><span class="p">];</span>
<span class="n">Array</span><span class="p">.</span><span class="n">Fill</span><span class="p">&lt;</span><span class="kt">float</span><span class="p">&gt;(</span> <span class="n">arr</span><span class="p">,</span> <span class="m">23.74f</span> <span class="p">);</span>
	
<span class="kt">var</span> <span class="n">v8sum</span> <span class="p">=</span> <span class="k">new</span> <span class="n">Vector</span><span class="p">&lt;</span><span class="kt">float</span><span class="p">&gt;();</span>

<span class="k">for</span><span class="p">(</span> <span class="kt">int</span> <span class="n">i</span> <span class="p">=</span> <span class="m">0</span><span class="p">;</span> <span class="n">i</span> <span class="p">&lt;</span> <span class="n">arr</span><span class="p">.</span><span class="n">Length</span><span class="p">;</span> <span class="n">i</span><span class="p">+=</span><span class="n">lanes</span> <span class="p">)</span>
<span class="p">{</span>
	<span class="kt">var</span> <span class="n">v8temp</span> <span class="p">=</span> <span class="k">new</span> <span class="n">Vector</span><span class="p">&lt;</span><span class="kt">float</span><span class="p">&gt;(</span> <span class="n">arr</span><span class="p">,</span> <span class="n">i</span> <span class="p">);</span>
	<span class="n">v8sum</span> <span class="p">+=</span> <span class="n">v8temp</span><span class="p">;</span>
<span class="p">}</span>
</pre></table></code></div></div><p>Note how inside if condition we don’t operate on any specific size corresponding to SIMD lanes width. It’s resolved at runtime and once it’s written we don’t need to worry about it (on my machine <code class="language-plaintext highlighter-rouge">v8sum</code> vector resolved to 8 elements).</p><ul><li>Rust</ul><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="k">let</span> <span class="p">(</span><span class="k">mut</span> <span class="n">v8temp</span><span class="p">,</span> <span class="k">mut</span> <span class="n">v8sum</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span> <span class="nf">_mm256_setzero_ps</span><span class="p">(),</span> <span class="nf">_mm256_setzero_ps</span><span class="p">()</span> <span class="p">);</span>
<span class="k">let</span> <span class="n">arr</span><span class="p">:</span> <span class="p">[</span><span class="nb">f32</span><span class="p">;</span> <span class="mi">1_200_000</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">23.74</span><span class="p">;</span> <span class="mi">1_200_000</span> <span class="p">];</span>

<span class="c1">//first version</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">arr</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.step_by</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">v8temp</span> <span class="o">=</span> <span class="nf">_mm256_loadu_ps</span><span class="p">(</span> <span class="n">i</span> <span class="p">);</span>
	<span class="n">v8sum</span> <span class="o">=</span> <span class="nf">_mm256_add_ps</span><span class="p">(</span> <span class="n">v8sum</span><span class="p">,</span> <span class="n">v8temp</span> <span class="p">);</span>
<span class="p">}</span>
<span class="k">let</span> <span class="n">v8sum_unpacked</span><span class="p">:</span> <span class="p">(</span><span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">,</span> <span class="nb">f32</span><span class="p">)</span> <span class="o">=</span>  <span class="nn">std</span><span class="p">::</span><span class="nn">mem</span><span class="p">::</span><span class="nf">transmute</span><span class="p">(</span> <span class="n">v8sum</span> <span class="p">);</span>

<span class="c1">//second version</span>
<span class="n">arr</span><span class="nf">.chunks</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="nf">.for_each</span><span class="p">(|</span><span class="n">chunk</span><span class="p">|</span> <span class="p">{</span>
	<span class="n">v8temp</span> <span class="o">=</span> <span class="nf">_mm256_loadu_ps</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">chunk</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">);</span>
	<span class="n">v8sum</span> <span class="o">=</span> <span class="nf">_mm256_add_ps</span><span class="p">(</span> <span class="n">v8sum</span><span class="p">,</span> <span class="n">v8temp</span> <span class="p">);</span>
<span class="p">}</span> <span class="p">);</span>
<span class="k">let</span> <span class="n">v8sum_sliced</span> <span class="o">=</span> <span class="nn">std</span><span class="p">::</span><span class="nn">mem</span><span class="p">::</span><span class="nn">transmute</span><span class="p">::</span><span class="o">&lt;</span> <span class="n">__m256</span><span class="p">,</span> <span class="p">[</span><span class="nb">f32</span><span class="p">;</span> <span class="mi">8</span><span class="p">]</span> <span class="o">&gt;</span><span class="p">(</span><span class="n">v8sum</span><span class="p">);</span>
</pre></table></code></div></div><p>Rust is pretty straightforward if you are already familiar with syntax, but there are few things to note here.</p><p>Example shows 2 loops, one using iterators and one chunks. It was not clear at first but for faster computations we should go with functional style. <strong><em>For loop</em></strong> uses iterators which are indirection to data, this is pretty significant with high performance code. Chunks approach on the other hand operates on raw data, which results in faster compute times.</p><p>Another thing worth mentioning is <strong>mem::transmute</strong>. This fn is responsible for bitcasting and under the hood it’s just union type.</p><p>And last but not least fact is that arrays in rust do not hold aligment requirements by default. That’s why we need to use <code class="language-plaintext highlighter-rouge">_mm256_loadu_ps</code> instead of <code class="language-plaintext highlighter-rouge">_mm256_load_ps</code> (notice lack of <code class="language-plaintext highlighter-rouge">u</code>).</p><p>Once <strong>std::simd</strong> stabilization finishes we can expect more rusty syntax:</p><div lang="rust" class="language-rust highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">let</span> <span class="p">(</span><span class="k">mut</span> <span class="n">v8temp</span><span class="p">,</span> <span class="k">mut</span> <span class="n">v8sum</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span> <span class="nn">f32x8</span><span class="p">::</span><span class="nf">splat</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> <span class="nn">f32x8</span><span class="p">::</span><span class="nf">splat</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="p">);</span>
<span class="n">arr</span><span class="nf">.chunks</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="nf">.for_each</span><span class="p">(|</span><span class="n">chunk</span><span class="p">|</span> <span class="p">{</span>
	<span class="n">v8temp</span> <span class="o">=</span> <span class="nn">f32x8</span><span class="p">::</span><span class="nf">from_array</span><span class="p">(</span> <span class="n">chunk</span><span class="nf">.try_into</span><span class="p">()</span><span class="nf">.expect</span><span class="p">(</span><span class="s">""</span><span class="p">)</span> <span class="p">);</span>
	<span class="n">v8sum</span> <span class="o">+=</span> <span class="n">v8temp</span><span class="p">;</span>
<span class="p">});</span>
</pre></table></code></div></div><p>Notice how natural using <code class="language-plaintext highlighter-rouge">v8sum += v8temp</code> will be, those are just pure SIMD constructs. If you want to check outgoing efforts to stabilize it right now, add this as dependecy to your project:</p><div lang="bash" class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>core_simd <span class="o">=</span> <span class="o">{</span> git <span class="o">=</span> <span class="s2">"https://github.com/rust-lang/stdsimd"</span> <span class="o">}</span>
</pre></table></code></div></div><h3 id="compiler-flags">Compiler flags</h3><p>One more thing worth mentioning are compiler flags. While CLR doesn’t need anything, compilers for C++ and Rust must provide information what characteristics of CPU we want to activate. For GCC (unless it’s SSE which can be compiled right away) we need to pass <strong>-mavx</strong> or <strong>-march=native</strong>.</p><p>Rustc on the other hand needs specific rustflags in .cargo/config.toml:</p><div lang="toml" class="language-toml highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nn">[build]</span>
<span class="py">rustflags</span> <span class="p">=</span> <span class="p">[</span> <span class="s">"-C"</span><span class="p">,</span> <span class="py">"target-feature</span><span class="p">=</span><span class="err">+avx</span><span class="p">,</span><span class="err">+avx</span><span class="mi">2</span><span class="s">" ]</span><span class="err">
</span></pre></table></code></div></div><blockquote><p><strong>rustflags</strong> have precedence over <strong>attributes</strong> !</p></blockquote><blockquote><p>Although this post is not Windows focused it’s also worth to mention that since <em>Visual Studio 19</em> we can use MSVC switch <code class="language-plaintext highlighter-rouge">-openmp:experimental</code> which is superset of previously available <code class="language-plaintext highlighter-rouge">-openmp</code>. Now before loop you want to vectorize just add <code class="language-plaintext highlighter-rouge">#pragma omp simd simdlen(x)</code>. You can read more <a href="https://devblogs.microsoft.com/cppblog/simd-extension-to-c-openmp-in-visual-studio/">here</a> about other clauses that let you modify how this pragma works.</p></blockquote><h2 id="bonus---veryfing-assembly">Bonus - veryfing assembly</h2><p>As small bonus: let’s see how to check if our binaries are actually using AVX instructions. In C++ and Rust it’s pretty straightforward, we have already compiled blob so we can use well known tool called <strong>objdump</strong> (<em>readelf</em> or <em>nm</em> will also do, if that’s what you prefer):</p><div lang="bash" class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>objdump <span class="nt">-d</span> simd_exec <span class="o">&gt;</span> simd_exec.asm
<span class="nb">grep</span> <span class="s2">"ymm"</span> simd_exec.asm
</pre></table></code></div></div><p>in my case output looks like this:</p><pre><code class="language-asm">    a71e:	c5 fc 29 00          	vmovaps %ymm0,(%rax)
    ad08:	c5 fc 28 06          	vmovaps (%rsi),%ymm0
    ad0c:	c5 fc 58 02          	vaddps (%rdx),%ymm0,%ymm0
    ad10:	c5 fc 29 07          	vmovaps %ymm0,(%rdi)
</code></pre><p>Here we check for <strong>ymm</strong> presence in produced assembly; those are 256bit wide registers used for AVX and AVX2. We can also grep for specific instruction like <strong>vaddps</strong> which uses ymm registers to execute addition.</p><p>C# is bit harder to check. When we compile our code, we don’t produce machine level assembly but rather MSIL (Microsoft Intermediate Language) that is later interpreted/compiled further at runtime. For SIMD produced MSIL has no use, we need machine instructions, which means we need to run our code on some specific processor. CLR will produce assembly that is no longer portable yet perfectly fitted for our hardware. If you are using Visual Studio Code there is really nice extension called <a href="https://github.com/jashook/vscode-dotnet-insights"><strong>.NET Insights</strong></a> where we can inspect our JITted code. Unfortunatelly as of writing this article it’s not crossplatform and only works on Windows machine. Here is output of SIMD code used in C#:</p><pre><code class="language-asm">G_M19891_IG04: ;; offset=0152H
000152  C4E34D18F701 vinsertf128 ymm6, ymm6, ymm7, 1
000158  C5CC58F0 vaddps ymm6, ymm6, ymm0
00015C  83C308  add  ebx, 8
00015F  81FB804F1200 cmp ebx, 0x124F80
000165  0F8CEFFEFFFF jl G_M19891_IG03
</code></pre><h2 id="footnotes">Footnotes</h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>I imagine this may happen on modern CPUs that support AVX-512 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/cpu/'>CPU</a>, <a href='/categories/simd/'>SIMD</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/cpu/" class="post-tag no-text-decoration" >cpu</a> <a href="/tags/simd/" class="post-tag no-text-decoration" >simd</a> <a href="/tags/cpp/" class="post-tag no-text-decoration" >cpp</a> <a href="/tags/csharp/" class="post-tag no-text-decoration" >csharp</a> <a href="/tags/rust/" class="post-tag no-text-decoration" >rust</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=SIMD usage in C++, C# and Rust - vkSegfault&url=https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=SIMD usage in C++, C# and Rust - vkSegfault&u=https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=SIMD usage in C++, C# and Rust - vkSegfault&url=https://vksegfault.github.io//posts/simd-usage-cpp-csharp-rust/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/godot-server-aws-azure/">Hosting Godot 4 server on AWS and Azure</a><li><a href="/posts/terraform-remote-backend/">Terraform remote backend</a><li><a href="/posts/java-simd/">SIMD usage in Java</a><li><a href="/posts/simd-usage-cpp-csharp-rust/">SIMD usage in C++, C# and Rust</a><li><a href="/posts/gentle-intro-gpu-inner-workings/">Gentle introduction to GPUs inner workings</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/aws/">aws</a> <a class="post-tag" href="/tags/cpu/">cpu</a> <a class="post-tag" href="/tags/simd/">simd</a> <a class="post-tag" href="/tags/terraform/">terraform</a> <a class="post-tag" href="/tags/azure/">azure</a> <a class="post-tag" href="/tags/boto3/">boto3</a> <a class="post-tag" href="/tags/cpp/">cpp</a> <a class="post-tag" href="/tags/csharp/">csharp</a> <a class="post-tag" href="/tags/godot/">godot</a> <a class="post-tag" href="/tags/gpu/">gpu</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/java-simd/"><div class="card-body"> <span class="timeago small" >Oct 13, 2021<i class="unloaded">2021-10-13T00:00:00+02:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>SIMD usage in Java</h3><div class="text-muted small"><p> You can consider this post as an extension to previously written SIMD usage in C++, C# and Rust. Example verified on OpenJDK 17 build running on Linux. Seems that CPU intrinsics are no lo...</p></div></div></a></div><div class="card"> <a href="/posts/terraform-remote-backend/"><div class="card-body"> <span class="timeago small" >Mar 14<i class="unloaded">2023-03-14T00:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Terraform remote backend</h3><div class="text-muted small"><p> It is usually a case that for very small (and sometimes even for medium-sized projects) we store our Terraform state file(s) as a part of our repo - in other words we use local state. While this mi...</p></div></div></a></div><div class="card"> <a href="/posts/godot-server-aws-azure/"><div class="card-body"> <span class="timeago small" >Nov 27, 2022<i class="unloaded">2022-11-27T00:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Hosting Godot 4 server on AWS and Azure</h3><div class="text-muted small"><p> Godot is slowly1 getting to release of version 4 - probably biggest update to date. Most interesting from our perspective is rewritten networking part. While there is stable Godot 3 relase as well,...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></span> <a href="/posts/gentle-intro-gpu-inner-workings/" class="btn btn-outline-primary" prompt="Newer"><p>Gentle introduction to GPUs inner workings</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://www.linkedin.com/in/adrian-jurczak-530aba192/">Adrian Jurczak</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/aws/">aws</a> <a class="post-tag" href="/tags/cpu/">cpu</a> <a class="post-tag" href="/tags/simd/">simd</a> <a class="post-tag" href="/tags/terraform/">terraform</a> <a class="post-tag" href="/tags/azure/">azure</a> <a class="post-tag" href="/tags/boto3/">boto3</a> <a class="post-tag" href="/tags/cpp/">cpp</a> <a class="post-tag" href="/tags/csharp/">csharp</a> <a class="post-tag" href="/tags/godot/">godot</a> <a class="post-tag" href="/tags/gpu/">gpu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://vksegfault.github.io/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-RSW0NEDL8V"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-RSW0NEDL8V'); }); </script>
